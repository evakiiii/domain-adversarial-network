{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e577d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#random.seed(525)\n",
    "#random.seed(3407)\n",
    "\n",
    "lr = 0.0001                 \n",
    "batchsize = 64            \n",
    "epochs = 400          \n",
    "\n",
    "weight_L2norm = 0.05     #mmd\n",
    "weight_entropy = 0.1     #条件熵\n",
    "lambda1 = 1          #交叉-类内\n",
    "lambda2 = 1          #交叉-类间\n",
    "c = 1               #分类器\n",
    "d = 1               #域判别器\n",
    "gamma = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e8ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def datapreprocessing(data):     ###min-max\n",
    "    dataset = data.iloc[:,:-1]        \n",
    "    label = data.iloc[:,-1].values   \n",
    "    data_temp = dataset.values \n",
    "    \n",
    "    #dataset = (dataset/np.sqrt(sum(data_temp*data_temp))).values  \n",
    "    min_values = np.min(data_temp, axis=0)\n",
    "    max_values = np.max(data_temp, axis=0)\n",
    "    max_values[max_values == min_values] = 1\n",
    "    dataset = (data_temp - min_values) / (max_values - min_values)\n",
    "    return dataset,label            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dfaec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, constant):\n",
    "        ctx.constant = constant\n",
    "        return x\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.neg() * ctx.constant     ## .neg(), Returns the input tensor as negative by element, out=−1∗input\n",
    "        return grad_output, None\n",
    "\n",
    "    def grad_reverse(x, constant):\n",
    "        return GradReverse.apply(x, constant)\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.feature = nn.Linear(128,200)               ## input layer, according to the data format\n",
    "        self.batch = nn.BatchNorm1d(200)                ## normalization\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.feature(x))                     ## The activation function relu\n",
    "        x = self.batch(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Class_classifier1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Class_classifier1, self).__init__()\n",
    "        self.d = nn.Linear(200, 200)\n",
    "        self.g = nn.Linear(200, 200)  \n",
    "        self.out = nn.Linear(200, 6)\n",
    "\n",
    "    def gate(self, whichinput):\n",
    "        return self.g(whichinput)\n",
    "\n",
    "    def forward(self, x, whichinput):\n",
    "        g = self.gate(whichinput)\n",
    "        g = F.sigmoid(g)  \n",
    "        \n",
    "        d = self.d(x)         \n",
    "        \n",
    "        h = d * g + x * (1 - g)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        out1 = self.out(h)\n",
    "        return F.log_softmax(out1, dim=1), out1\n",
    "    \n",
    "    \n",
    "class Class_classifier2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Class_classifier2, self).__init__()\n",
    "        self.d1 = nn.Linear(200, 200)\n",
    "        self.d2 = nn.Linear(200, 200)\n",
    "        self.d3 = nn.Linear(200, 200)\n",
    "        self.d4 = nn.Linear(200, 200)\n",
    "        self.d5 = nn.Linear(200, 200)\n",
    "        \n",
    "        self.g = nn.Linear(200, 200)  # 用于计算门控值\n",
    "        self.out = nn.Linear(200, 6)\n",
    "        self.dropout = nn.Dropout(0.5)  # 添加dropout层\n",
    "\n",
    "    def gate(self, whichinput):\n",
    "        return self.g(whichinput)\n",
    "\n",
    "    def forward(self, x, whichinput):\n",
    "        g = self.gate(whichinput)\n",
    "        g = F.sigmoid(g)  \n",
    "        \n",
    "        d = self.d1(x)\n",
    "        d = self.d2(d)\n",
    "        d = self.d3(d)\n",
    "        d = self.d4(d)\n",
    "        d = self.d5(d)\n",
    "        \n",
    "        \n",
    "        h = d * g + x * (1 - g)  # 使用门控机制\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        out1 = self.out(h)\n",
    "        return F.log_softmax(out1, dim=1), out1\n",
    "            \n",
    "class Domain_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Domain_classifier, self).__init__()\n",
    "        # self.domian_out1 = nn.Linear(200,100)\n",
    "        self.domian_out2 = nn.Linear(200,2)      ## domain label source domain as 0 and target as 1\n",
    "\n",
    "    def forward(self,x,constant):\n",
    "        input = GradReverse.grad_reverse(x,constant)  ##  reverse the gradient.\n",
    "        # input = F.relu(self.domian_out1(input))\n",
    "        return F.log_softmax(self.domian_out2(input),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73395702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def optimizer_scheduler(optimizer, p):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75 \n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f72b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import choice\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "# Calculate the L_f\n",
    "def get_L2Norm_loss(x):\n",
    "    redius = x.norm(p=2,dim=1).detach()\n",
    "    assert redius.requires_grad == False\n",
    "    redius +=redius + 1.0\n",
    "    l = ((x.norm(p=2,dim=1)-redius)**2).mean()\n",
    "    return weight_L2norm*l\n",
    "\n",
    "# Calculate the conditional entropy of target domain\n",
    "def get_entropy_loss(p_softmax):\n",
    "    mask = p_softmax.ge(0.000001)\n",
    "    mask_out = torch.masked_select(p_softmax,mask)\n",
    "    entropy = -(torch.sum(mask_out*torch.log(mask_out)))\n",
    "    return weight_entropy*(entropy/float(p_softmax.size(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "540a19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################交叉伪标签###############交叉s\n",
    "def intra_class_compactness(X, y):\n",
    "    compactness = 0\n",
    "    unique_classes = np.unique(y)\n",
    "    for cls in unique_classes:#遍历唯一的类标签\n",
    "        X_cls = X[np.where(y == cls)[0]]  # 将布尔掩码转换为一维数组形式\n",
    "        if len(X_cls) > 1:\n",
    "            compactness += np.mean(np.linalg.norm(X_cls - np.mean(X_cls, axis=0), axis=1))#每个类内数据点到类均值的平均距离的均值\n",
    "    return compactness / len(unique_classes)\n",
    "def inter_class_dispersion(X, y):\n",
    "    centroids = []#每个类别的中心点\n",
    "    unique_classes = np.unique(y)\n",
    "    for cls in unique_classes:\n",
    "        X_cls = X[np.where(y == cls)[0]]\n",
    "        centroids.append(np.mean(X_cls, axis=0))\n",
    "    centroids = np.array(centroids)#计算每个类别的中心点\n",
    "    dispersion = np.mean(np.linalg.norm(centroids - np.mean(centroids, axis=0), axis=1))#计算这些中心点与所有中心点的整体中心点之间的平均距离\n",
    "    return dispersion\n",
    "\n",
    "def cross_class_rejection_loss(X, y, lambda1, lambda2):\n",
    "    X = X.detach().numpy()\n",
    "    y = y.detach().numpy()\n",
    "    \n",
    "    compactness = intra_class_compactness(X, y)\n",
    "    dispersion = inter_class_dispersion(X, y)\n",
    "    loss = lambda1 * compactness - lambda2 * dispersion\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154a0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW(probs1, probs2):\n",
    "    k = 2\n",
    "    gailv1, indices1 = torch.topk(probs1, k, dim=1, largest=True, sorted=True)\n",
    "    gailv2, indices2 = torch.topk(probs2, k, dim=1, largest=True, sorted=True)    \n",
    "    diff1 = gailv1[:, 0] - gailv1[:, 1]\n",
    "    diff2 = gailv2[:, 0] - gailv2[:, 1]\n",
    "    diff = diff1+diff2    \n",
    "    CW1 = diff1/diff\n",
    "    CW2 = diff2/diff     \n",
    "    CW1 = torch.mean(CW1)\n",
    "    CW2 = torch.mean(CW2)    \n",
    "    return CW1, CW2  #return mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6281b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Local_dclassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Local_dclassifier, self).__init__()\n",
    "        self.classes = 6\n",
    "        self.dci = nn.ModuleList()\n",
    "        for i in range(self.classes):\n",
    "            classifier = nn.Sequential(nn.Linear(200, 2))\n",
    "            self.dci.append(classifier)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        source_feature, target_feature, alpha, src_probs, tgt_probs = x\n",
    "        p_source = src_probs                #source label的概率（0-1）   64,6----------source feature probs \n",
    "        \n",
    "        p_target = tgt_probs                #target label的概率（0-1）----------target feature probs        \n",
    "        t_label = tgt_probs.data.max(1)[1]               #返回target label大到小     64,6\n",
    "        \n",
    "        s_out = []\n",
    "        t_out = []\n",
    "       \n",
    "        # RevGrad\n",
    "        s_reverse_feature = GradReverse.apply(source_feature, alpha)    #src feature经过反转层64,200\n",
    "        t_reverse_feature = GradReverse.apply(target_feature, alpha)    #tgt feature经过反转层64,200\n",
    "        \n",
    "        # p*feature-> classifier_i ->loss_i\n",
    "        for i in range(self.classes):#6种气体\n",
    "            ps = p_source[:, i].reshape((target_feature.shape[0],1))#64,6->64,1 源域分类概率拿出每一列：一个标签下的所有概率\n",
    "            fs = ps * s_reverse_feature#64,200  feature在每一类中*每个样本的概率（注意力）\n",
    "            pt = p_target[:, i].reshape((target_feature.shape[0],1))\n",
    "            ft = pt * t_reverse_feature\n",
    "            \n",
    "            outsi = self.dci[i](fs)#加入注意力的反转srcfeature经过local domain，在类内做domain区分，每类都是单独的神经网络64，2\n",
    "            s_out.append(outsi)#6个outsi\n",
    "            outti = self.dci[i](ft)#加入注意力的反转tgtfeature经过local domain，在类内做domain区分，每类都是单独的神经网络64，2\n",
    "            t_out.append(outti)\n",
    "   \n",
    "        return s_out, t_out##domain结果（st的local domain）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa32761",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def mmd_linear(X, Y):\n",
    "    delta = X.mean(0) - Y.mean(0)\n",
    "    return delta.dot(delta.T)\n",
    " \n",
    "def mmd_rbf(X, Y, gamma=1.0):\n",
    "    XX = metrics.pairwise.rbf_kernel(X, X, gamma)\n",
    "    YY = metrics.pairwise.rbf_kernel(Y, Y, gamma)\n",
    "    XY = metrics.pairwise.rbf_kernel(X, Y, gamma)\n",
    "    return XX.mean() + YY.mean() - 2 * XY.mean()\n",
    " \n",
    "def mmd_poly(X, Y, degree=2, gamma=1, coef0=0):\n",
    "    XX = metrics.pairwise.polynomial_kernel(X, X, degree, gamma, coef0)\n",
    "    YY = metrics.pairwise.polynomial_kernel(Y, Y, degree, gamma, coef0)\n",
    "    XY = metrics.pairwise.polynomial_kernel(X, Y, degree, gamma, coef0)\n",
    "    return XX.mean() + YY.mean() - 2 * XY.mean()\n",
    "\n",
    "def CORAL(source, target, **kwargs):\n",
    "    d = source.data.shape[1]\n",
    "    ns, nt = source.data.shape[0], target.data.shape[0]\n",
    "    # source covariance\n",
    "    xm = torch.mean(source, 0, keepdim=True) - source\n",
    "    xc = xm.t() @ xm / (ns - 1)\n",
    "\n",
    "    # target covariance\n",
    "    xmt = torch.mean(target, 0, keepdim=True) - target\n",
    "    xct = xmt.t() @ xmt / (nt - 1)\n",
    "\n",
    "    # frobenius norm between source and target\n",
    "    loss = torch.mul((xc - xct), (xc - xct))\n",
    "    loss = torch.sum(loss) / (4*d*d)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b410728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(feature_extractor,class_classifier1,class_classifier2,\n",
    "          domain_classifier,local_dclassifier,class_criterion,domain_criterion,\n",
    "          source_dataloader,target_dataloader,optimizer,epoch,\n",
    "          local_w,num_class):\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    feature_extractor.train()\n",
    "    class_classifier1.train()\n",
    "    class_classifier2.train()\n",
    "    domain_classifier.train()\n",
    "    local_dclassifier.train()\n",
    "\n",
    "    # Set the experimental step\n",
    "    start_steps = epoch * len(source_dataloader)          ## start step\n",
    "    total_steps = epochs * len(source_dataloader)  ## total steps\n",
    "    \n",
    "    global D_M, D_C, MU\n",
    "   \n",
    "    d_c = 0\n",
    "    d_m = 0\n",
    "    wc_list = [0] * num_class#积累一个epoch中：每个类别的Ll\n",
    "    wc_daoshu = [0] * num_class\n",
    "    '''update mu per epoch''' \n",
    "    \n",
    "    if D_M==0 and D_C==0 and MU==0:\n",
    "        MU = 0.5\n",
    "    elif D_M != 0 or D_C != 0:\n",
    "        if D_M + D_C ==0:\n",
    "            MU = 0.5\n",
    "        else:\n",
    "            D_M = D_M/len(source_dataloader)\n",
    "            D_C = D_C/len(source_dataloader)\n",
    "            MU = D_M/(D_M + D_C)\n",
    "\n",
    "        \n",
    "    for batch_idx,(sdata,tdata) in enumerate(zip(source_dataloader,target_dataloader)):  ## loop training, the standard PyTorch training mode\n",
    "        if len(sdata[0]) != len(tdata[0]):#b10、9、7、6、3、2=6,b8=4，b5=3，b4=2\n",
    "            break\n",
    "        p = float(batch_idx + start_steps) / total_steps     ##  a variable for adjusting learning rate \n",
    "        constant = 2./(1+np.exp(-gamma*p))-1      ##  a constant of RevGrad\n",
    "\n",
    "        # Get data for the source and target domains\n",
    "        input1,label1 = sdata\n",
    "        input2,label2 = tdata\n",
    "        \n",
    "\n",
    "        # Dynamically adjust the learning rate, the standard format for PyTorch\n",
    "        optimizer = optimizer_scheduler(optimizer,p)\n",
    "        optimizer.zero_grad()                                   \n",
    "\n",
    "        # Set the domain label to 0 for the source domain and 1 for the target domain\n",
    "        source_labels = Variable(torch.zeros(input1.size()[0])).long()  ##  source label 0   (domain label)\n",
    "        target_labels = Variable(torch.ones(input2.size()[0])).long()   ##  target label 1   (domain label)\n",
    "\n",
    "        ## Feature extraction using feature extractor \n",
    "        src_feature = feature_extractor(input1)    ## source\n",
    "        tgt_feature = feature_extractor(input2)    ## target\n",
    "        src_mean = torch.mean(src_feature, dim=0)\n",
    "        \n",
    "        delta1 = src_feature - src_mean\n",
    "        delta2 = tgt_feature - src_mean\n",
    "        \n",
    "        cross_loss = cross_class_rejection_loss(src_feature, label1, lambda1, lambda2)#\n",
    "           \n",
    "        ######################### compute the class loss of src_feature  \n",
    "        class_preds1,s_logits1= class_classifier1(src_feature, delta1)\n",
    "        class_loss1 = class_criterion(class_preds1, label1)     \n",
    "        class_preds2,s_logits2= class_classifier2(src_feature, delta1)  \n",
    "        class_loss2 = class_criterion(class_preds2, label1)                 \n",
    "        class_loss = class_loss1 + class_loss2 \n",
    "            \n",
    "            \n",
    "        ########################## Calculate the pseudo label &&&&&&&& the entropy of the target domain\n",
    "        toutput1, tlogits1 = class_classifier1(tgt_feature, delta2)\n",
    "        toutput2, tlogits2 = class_classifier2(tgt_feature, delta2)         \n",
    "        tprobs1=F.softmax(tlogits1)\n",
    "        tprobs2=F.softmax(tlogits2)\n",
    "        t_entropy_loss = get_entropy_loss(tprobs1) +get_entropy_loss(tprobs2)\n",
    "        \n",
    "        tcon1, tcon2 = CW(tprobs1, tprobs2)#计算第i个迭代平均置信度\n",
    "        tcon = tcon1**2 + tcon2**2\n",
    "        tweight1 = tcon1 ** 2 / tcon\n",
    "        tweight2 = tcon2 ** 2 / tcon    \n",
    "        tgt_probs = tweight1 * tprobs1 + tweight2 * tprobs2\n",
    "        \n",
    "        \n",
    "        soutput1, slogits1 = class_classifier1(src_feature, delta1)\n",
    "        soutput2, slogits2 = class_classifier2(src_feature, delta1)         \n",
    "        sprobs1=F.softmax(slogits1)\n",
    "        sprobs2=F.softmax(slogits2)\n",
    "        \n",
    "        scon1, scon2 = CW(sprobs1, sprobs2)#计算第i个迭代平均置信度\n",
    "        scon = scon1**2 + scon2**2\n",
    "        sweight1 = scon1 ** 2 / scon\n",
    "        sweight2 = scon2 ** 2 / scon    \n",
    "        src_probs = sweight1 * sprobs1 + sweight2 * sprobs2\n",
    "############################################################################################\n",
    "        # Compute domain adversarial losses\n",
    "        tgt_preds = domain_classifier(tgt_feature,constant)\n",
    "        src_preds = domain_classifier(src_feature,constant)\n",
    "        tgt_loss = domain_criterion(tgt_preds,target_labels)   ## target loss\n",
    "        src_loss = domain_criterion(src_preds,source_labels)   ## source loss\n",
    "        domain_loss = tgt_loss + src_loss                      ## domain loss = target adversarial loss + source adversarial loss  \n",
    "        \n",
    "            \n",
    "        ##################### Calculate the local domain #####################\n",
    "        p1 = float(batch_idx) / len(target_dataloader)\n",
    "        constant1 = 2. / (1. + np.exp(-10*p)) - 1\n",
    "\n",
    "        canshu = [src_feature, tgt_feature, constant1, src_probs, tgt_probs]\n",
    "        src_local_domain,tgt_local_domain = local_dclassifier(canshu)\n",
    "        \n",
    "        \n",
    "        loss_s = 0.0\n",
    "        loss_t = 0.0\n",
    "        tmpd_c = 0.0\n",
    "        tmpd_c_daoshu = 0.0\n",
    "        epsilon = 1e-8   \n",
    "        for i in range(num_class):#每一类加过注意力得到的domain标签归一化，然后log一下\n",
    "            loss_si = F.nll_loss(F.log_softmax(src_local_domain[i], dim=1), source_labels)\n",
    "            loss_ti = F.nll_loss(F.log_softmax(tgt_local_domain[i], dim=1), target_labels)\n",
    "            \n",
    "            loss_s += loss_si * local_w[i]\n",
    "            loss_t += loss_ti * local_w[i]\n",
    "            \n",
    "            wc_list[i] += (2 * (1 - 2 * (loss_si + loss_ti))).item()#3个dc\n",
    "            wc_daoshu[i] = 1 / wc_list[i] if abs(wc_list[i]) > epsilon else 0#3个dc的倒数\n",
    "        \n",
    "            tmpd_c += 2 * (1 - 2 * (loss_si + loss_ti))\n",
    "            \n",
    "        tmpd_c /= num_class#1/C求和dc\n",
    "        tmpd_c_daoshu = sum(wc_daoshu)/num_class#1/C求和1/dc\n",
    "        #print(wc_list)#3个dc  \n",
    "        #print(wc_daoshu)#3个倒数dc \n",
    "        #print(tmpd_c)#dc的和\n",
    "        #print(tmpd_c_daoshu)#倒数dc的和\n",
    "        #print(D_M)\n",
    "        #print(D_C)\n",
    "        #print(MU)\n",
    "        #print(local_w)\n",
    "            \n",
    "        global_loss = 0.1 * domain_loss\n",
    "        local_loss = 0.05*(loss_s + loss_t)  \n",
    "        \n",
    "        d_c = d_c + tmpd_c.cpu().item()#累积每个batch的（1/C求和dc）\n",
    "        d_m = d_m + 2 * (1 - 2 * global_loss.cpu().item())#累积每个batch的dg\n",
    "           \n",
    "        join_loss = (1 - MU) * global_loss + MU * local_loss\n",
    "        \n",
    " ############################################################################################################################## \n",
    "        loss = c * class_loss + d * join_loss + t_entropy_loss + cross_loss \n",
    "        loss.backward()                # the standard PyTorch training mode     \n",
    "        optimizer.step()               # the standard PyTorch training mode\n",
    "    \n",
    "    D_M = np.copy(d_m).item()\n",
    "    D_C = np.copy(d_c).item()\n",
    "    for j in range(num_class):#change every batch\n",
    "        local_w[j] = wc_daoshu[j]/tmpd_c_daoshu   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1522234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "def test(feature_extractor,class_classifier1,class_classifier2,domain_classifier,local_dclassifier,source_dataloader,target_dataloader):\n",
    "    \n",
    "    ## The standard PyTorch test mode\n",
    "    feature_extractor.eval()\n",
    "    class_classifier1.eval()\n",
    "    class_classifier2.eval()\n",
    "    domain_classifier.eval()\n",
    "    local_dclassifier.eval()    \n",
    "    \n",
    "    target_correct = 0.0  ## target\n",
    "    tgt_correct = 0.0     ## target\n",
    "    geshu2= 0\n",
    "    \n",
    "    for batch_idx,(sdata,tdata) in enumerate(zip(source_dataloader,target_dataloader)):  \n",
    "        if len(sdata[0]) != len(tdata[0]):#b10、9、7、6、3、2=6,b8=4，b5=3，b4=2\n",
    "            break\n",
    "        p = float(batch_idx) / len(source_dataloader)\n",
    "        constant = 2. / (1. + np.exp(-10*p)) - 1\n",
    "\n",
    "        input1,label1 = sdata                             ## obtain the source data and labels\n",
    "        input2,label2 = tdata\n",
    "        src_feature = feature_extractor(input1)    ## source\n",
    "        tgt_feature = feature_extractor(input2)    ## target\n",
    "        src_mean = torch.mean(src_feature, dim=0)\n",
    "        delta = tgt_feature - src_mean\n",
    "        \n",
    "        tgt_labels = Variable(torch.ones((input2.size()[0])).type(torch.LongTensor))\n",
    "########################################  \n",
    "        output1, logits1 = class_classifier1(tgt_feature,delta)\n",
    "        output2, logits2 = class_classifier2(tgt_feature,delta)         \n",
    "        probs1 = torch.softmax(logits1, dim=1)\n",
    "        probs2 = torch.softmax(logits2, dim=1)\n",
    "        \n",
    "        con1, con2 = CW(probs1, probs2)#计算第i个迭代平均置信度\n",
    "        con = con1 + con2\n",
    "        \n",
    "        weight1 = (con1 ** 2)/ con\n",
    "        weight2 = (con2 ** 2) / con\n",
    "        weight = weight1 + weight2\n",
    "        weight1 = weight1 / weight\n",
    "        weight2 = weight2 / weight\n",
    "        \n",
    "        probs = weight1 * probs1 + weight2 * probs2\n",
    "        final_pred2 = probs.argmax(dim=1,keepdim=True)\n",
    "###########################################    \n",
    "        target_correct += final_pred2.eq(label2.data.view_as(final_pred2)).cpu().sum()\n",
    "        \n",
    "        tgt_preds = domain_classifier(tgt_feature,constant)\n",
    "        tgt_preds = tgt_preds.argmax(dim=1,keepdim=True)\n",
    "        tgt_correct += tgt_preds.eq(tgt_labels.data.view_as(tgt_preds)).cpu().sum()\n",
    "        \n",
    "        geshu2 += final_pred2.shape[0]\n",
    "    target_accuracy = 100. * float(target_correct)/geshu2  ## total prediction accuracy of target domain\n",
    "    return target_accuracy    ## return the target accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d11a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import argparse,sys,os\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.space import Real, Categorical\n",
    "\n",
    "\n",
    "\n",
    "def main(FeatureExtractor,Class_classifier1,Class_classifier2,\n",
    "         Domain_classifier,Local_dclassifier,\n",
    "         train_loader,test_loader,num_class):\n",
    "   \n",
    "    src_train_dataloader = train_loader   \n",
    "    src_test_dataloader  = train_loader   \n",
    "    tgt_test_dataloader  = test_loader    \n",
    "    tgt_train_dataloader = test_loader \n",
    "    \n",
    "    feature_extractor = FeatureExtractor\n",
    "    class_classifier1 = Class_classifier1\n",
    "    class_classifier2 = Class_classifier2\n",
    "    domain_classifier = Domain_classifier\n",
    "    local_dclassifier = Local_dclassifier\n",
    "\n",
    "    class_criterion = nn.NLLLoss()        \n",
    "    domain_criterion = nn.NLLLoss()       \n",
    "     \n",
    "    #optimizer_scheduler\n",
    "    optimizer = optim.SGD([{'params': feature_extractor.parameters()},\n",
    "                           {'params': class_classifier1.parameters()},\n",
    "                           {'params': class_classifier2.parameters()},\n",
    "                           {'params': domain_classifier.parameters()},\n",
    "                           {'params': local_dclassifier.parameters()}\n",
    "                         ], lr=lr,momentum=0.9)\n",
    "    acc_best = 0.0\n",
    "    local_w = [1] * num_class\n",
    "    for epoch in range(epochs):\n",
    "        #print(\"Epoch: {}\".format(epoch))\n",
    "        train(feature_extractor,class_classifier1,class_classifier2,\n",
    "              domain_classifier,local_dclassifier,class_criterion,domain_criterion,\n",
    "              src_train_dataloader,tgt_train_dataloader,optimizer,epoch,\n",
    "              local_w,num_class)\n",
    "        accuracy = test(feature_extractor,class_classifier1,class_classifier2,\n",
    "                        domain_classifier,local_dclassifier,\n",
    "                        src_test_dataloader,tgt_test_dataloader)\n",
    "        if accuracy > acc_best:\n",
    "            acc_best = accuracy\n",
    "        \n",
    "    print('best accuracy:',acc_best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06bda0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch2.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)\n",
    "\n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db2b3532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "global D_M, D_C, MU\n",
    "D_M = 0\n",
    "D_C = 0\n",
    "MU = 0  \n",
    "class_classifier1 = Class_classifier1()\n",
    "\n",
    "class_classifier2 = Class_classifier2()\n",
    "feature_extractor = FeatureExtractor()\n",
    "domain_classifier = Domain_classifier()\n",
    "local_dclassifier = Local_dclassifier()\n",
    "train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d3486d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb9ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e3f59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch3.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)  \n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e164be09",
   "metadata": {},
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82812a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bfbe027",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch4.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)  \n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b89dc87",
   "metadata": {},
   ],
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403d31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11a979d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch5.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)  \n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a40407e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de39997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "874af40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch6.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)  \n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4278d7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22908abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b8a2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch7.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)  \n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a4f2bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc860d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b432ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch8.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)  \n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cca9a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc08c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97a1d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch9.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)  \n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "834707cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bccacc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf55b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataS = pd.read_csv(r'E:/meeting/111111111111dataset/Dataset/batch1.csv',header=None)  \n",
    "dataT = pd.read_csv(r\"E:/meeting/111111111111dataset/Dataset/batch10.csv\",header=None) \n",
    "batchS,batchS_label = datapreprocessing(dataS)                                                     \n",
    "batchT,batchT_label = datapreprocessing(dataT)  \n",
    "input_data = torch.FloatTensor(batchS)                                                            \n",
    "label = torch.LongTensor(batchS_label-1)                                                   \n",
    "input_data1 = torch.FloatTensor(batchT)\n",
    "label1 = torch.LongTensor(batchT_label-1)\n",
    "\n",
    "train_dataset = Data.TensorDataset(input_data,label)                                             \n",
    "train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)\n",
    "test_dataset = Data.TensorDataset(input_data1,label1)\n",
    "test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "161283fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    global D_M, D_C, MU\n",
    "    D_M = 0\n",
    "    D_C = 0\n",
    "    MU = 0    \n",
    "    class_classifier1 = Class_classifier1()\n",
    "    class_classifier2 = Class_classifier2()\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    domain_classifier = Domain_classifier()\n",
    "    local_dclassifier = Local_dclassifier()\n",
    "    train_loader,test_loader=train_loader,test_loader \n",
    "\n",
    "    main(feature_extractor,class_classifier1,class_classifier2,\n",
    "     domain_classifier,local_dclassifier,train_loader,test_loader,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e690479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
