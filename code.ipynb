    lr = 0.0001                 
    batchsize = 64          
    epochs = 400       
    weight_L2norm = 0.05     
    weight_entropy = 0.1     
    lambda1 = 1          
    lambda2 = 1          
    c = 1               
    d = 1               
    gamma = 10 
 

    import torch
    import torch as t
    import torch.nn as nn
    import numpy as np
    import torch.nn.functional as F
    import pandas as pd
    import torch.utils.data as Data
    from torch.autograd import Variable
    from sklearn.model_selection import train_test_split
    from torch.utils.data import TensorDataset, DataLoader
    
    def datapreprocessing(data):    
        dataset = data.iloc[:,:-1]        
        label = data.iloc[:,-1].values  
        data_temp = dataset.values 

        #dataset = (dataset/np.sqrt(sum(data_temp*data_temp))).values  
        min_values = np.min(data_temp, axis=0)
        max_values = np.max(data_temp, axis=0)
        max_values[max_values == min_values] = 1
        dataset = (data_temp - min_values) / (max_values - min_values)
        return dataset,label            
  

    
    class GradReverse(torch.autograd.Function):
        @staticmethod
        def forward(ctx, x, constant):
           ctx.constant = constant
            return x

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.neg() * ctx.constant     ## .neg(), Returns the input tensor as negative by element, out=−1∗input\n",
            return grad_output, None
        def grad_reverse(x, constant):
           return GradReverse.apply(x, constant)

    class FeatureExtractor(nn.Module):
       def __init__(self):
            super(FeatureExtractor, self).__init__()
            self.feature = nn.Linear(128,200)               ## input layer, according to the data format
            self.batch = nn.BatchNorm1d(200)               


        def forward(self,x):\n",
            x = F.relu(self.feature(x))                    
            x = self.batch(x)
            return x

    class Class_classifier1(nn.Module):
        def __init__(self):
            super(Class_classifier1, self).__init__()
            self.d = nn.Linear(200, 200)
            self.g = nn.Linear(200, 200)  
            self.out = nn.Linear(200, 6)


        def gate(self, whichinput):
            return self.g(whichinput)

        def forward(self, x, whichinput):
            g = self.gate(whichinput)
            g = F.sigmoid(g)  
            d = self.d(x)        
            h = d * g + x * (1 - g)
            h = F.relu(h)

            out1 = self.out(h)
            return F.log_softmax(out1, dim=1), out1

    class Class_classifier2(nn.Module):
        def __init__(self):
            super(Class_classifier2, self).__init__()
            self.d1 = nn.Linear(200, 200)
            self.d2 = nn.Linear(200, 200)
            self.d3 = nn.Linear(200, 200)
            self.d4 = nn.Linear(200, 200)
            self.d5 = nn.Linear(200, 200)

            self.g = nn.Linear(200, 200)  # 用于计算门控值
            self.out = nn.Linear(200, 6)
            self.dropout = nn.Dropout(0.5)  # 添加dropout层

        def gate(self, whichinput):
            return self.g(whichinput)

        def forward(self, x, whichinput):
            g = self.gate(whichinput)
            g = F.sigmoid(g)  
            d = self.d1(x)
            d = self.d2(d)
            d = self.d3(d)
            d = self.d4(d)
            d = self.d5(d)

            h = d * g + x * (1 - g)  # 使用门控机制
            h = F.relu(h)

            out1 = self.out(h)
            return F.log_softmax(out1, dim=1), out1

    class Domain_classifier(nn.Module):
        def __init__(self):
            super(Domain_classifier, self).__init__()
            # self.domian_out1 = nn.Linear(200,100)
            self.domian_out2 = nn.Linear(200,2)      ## domain label source domain as 0 and target as 1

        def forward(self,x,constant):
            input = GradReverse.grad_reverse(x,constant)  ##  reverse the gradient
            # input = F.relu(self.domian_out1(input))
            return F.log_softmax(self.domian_out2(input),1)
   
    import torchvision\n",
    import torch\n",
    from torch.utils.data import DataLoader

    def optimizer_scheduler(optimizer, p):
        for param_group in optimizer.param_groups:
            param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75
        return optimizer"
   
   
     def get_entropy_loss(p_softmax):
        mask = p_softmax.ge(0.000001)
        mask_out = torch.masked_select(p_softmax,mask)
        entropy = -(torch.sum(mask_out*torch.log(mask_out)))
        return weight_entropy*(entropy/float(p_softmax.size(0)))

    def intra_class_compactness(X, y):
        compactness = 0
        unique_classes = np.unique(y)
        for cls in unique_classes:
            X_cls = X[np.where(y == cls)[0]]  
           if len(X_cls) > 1:
                compactness += np.mean(np.linalg.norm(X_cls - np.mean(X_cls, axis=0), axis=1))
        return compactness / len(unique_classes)
    def inter_class_dispersion(X, y):
        centroids = []#每个类别的中心点
        unique_classes = np.unique(y)
        for cls in unique_classes:
            X_cls = X[np.where(y == cls)[0]]
            centroids.append(np.mean(X_cls, axis=0))
        centroids = np.array(centroids)#计算每个类别的中心点
        dispersion = np.mean(np.linalg.norm(centroids - np.mean(centroids, axis=0), axis=1))#计算这些中心点与所有中心点的整体中心点之间的平均距离
        return dispersion

    def cross_class_rejection_loss(X, y, lambda1, lambda2):
        X = X.detach().numpy()
        y = y.detach().numpy()

        compactness = intra_class_compactness(X, y)
        dispersion = inter_class_dispersion(X, y)
        loss = lambda1 * compactness - lambda2 * dispersion
        return loss

    def CW(probs1, probs2):
        k = 2
        gailv1, indices1 = torch.topk(probs1, k, dim=1, largest=True, sorted=True)
        gailv2, indices2 = torch.topk(probs2, k, dim=1, largest=True, sorted=True)   
        diff1 = gailv1[:, 0] - gailv1[:, 1]
        diff2 = gailv2[:, 0] - gailv2[:, 1]
        diff = diff1+diff2   
        CW1 = diff1/diff
        CW2 = diff2/diff    
        CW1 = torch.mean(CW1)
        CW2 = torch.mean(CW2)    
        return CW1, CW2  #return mean value
  

    class Local_dclassifier(nn.Module):
        def __init__(self):
            super(Local_dclassifier, self).__init__()
            self.classes = 6
            self.dci = nn.ModuleList()
            for i in range(self.classes):
                classifier = nn.Sequential(nn.Linear(200, 2))
                self.dci.append(classifier)

        def forward(self, x):
            source_feature, target_feature, alpha, src_probs, tgt_probs = x
            p_source = src_probs               

            p_target = tgt_probs               
            t_label = tgt_probs.data.max(1)[1]              

            s_out = []
            t_out = []

            # RevGrad
            s_reverse_feature = GradReverse.apply(source_feature, alpha)    
            t_reverse_feature = GradReverse.apply(target_feature, alpha)    

            # p*feature-> classifier_i ->loss_i
           for i in range(self.classes):
                ps = p_source[:, i].reshape((target_feature.shape[0],1))#64,6->64,1 源域分类概率拿出每一列：一个标签下的所有概率
                fs = ps * s_reverse_feature#64,200  feature在每一类中*每个样本的概率（注意力）
                pt = p_target[:, i].reshape((target_feature.shape[0],1))
                ft = pt * t_reverse_feature

                outsi = self.dci[i](fs)#加入注意力的反转srcfeature经过local domain，在类内做domain区分，每类都是单独的神经网络64，2
                s_out.append(outsi)#6个outsi\n",
                outti = self.dci[i](ft)#加入注意力的反转tgtfeature经过local domain，在类内做domain区分，每类都是单独的神经网络64，2
                t_out.append(outti)
       
            return s_out, t_out##domain
  

  

    def train(feature_extractor,class_classifier1,class_classifier2,
              domain_classifier,local_dclassifier,class_criterion,domain_criterion,
              source_dataloader,target_dataloader,optimizer,epoch,
              local_w,num_class):
        # Set the model to training mode
        feature_extractor.train()
        class_classifier1.train()
        class_classifier2.train()
        domain_classifier.train()
        local_dclassifier.train()
        # Set the experimental step
        start_steps = epoch * len(source_dataloader)          ## start step
        total_steps = epochs * len(source_dataloader)  ## total steps
        global D_M, D_C, MU\n
       d_c = 0
        d_m = 0
        wc_list = [0] * num_class
        wc_daoshu = [0] * num_class
        '''update mu per epoch''' 
        if D_M==0 and D_C==0 and MU==0:
            MU = 0.5
        elif D_M != 0 or D_C != 0:
            if D_M + D_C ==0:
                MU = 0.5
            else:
                D_M = D_M/len(source_dataloader)
                D_C = D_C/len(source_dataloader)
                MU = D_M/(D_M + D_C)

        for batch_idx,(sdata,tdata) in enumerate(zip(source_dataloader,target_dataloader)): 
            p = float(batch_idx + start_steps) / total_steps     ##  a variable for adjusting learning rate 
            constant = 2./(1+np.exp(-gamma*p))-1      


            # Get data for the source and target domains
            input1,label1 = sdata
            input2,label2 = tdata

            optimizer = optimizer_scheduler(optimizer,p)
            optimizer.zero_grad()                                   

            source_labels = Variable(torch.zeros(input1.size()[0])).long()  ##  source label 0   (domain label)
            target_labels = Variable(torch.ones(input2.size()[0])).long()   ##  target label 1   (domain label)
            ## Feature extraction using feature extractor 
            src_feature = feature_extractor(input1)    
            tgt_feature = feature_extractor(input2)    
            src_mean = torch.mean(src_feature, dim=0)

            delta1 = src_feature - src_mean
            delta2 = tgt_feature - src_mean
            cross_loss = cross_class_rejection_loss(src_feature, label1, lambda1, lambda2)

            class_preds1,s_logits1= class_classifier1(src_feature, delta1)
            class_loss1 = class_criterion(class_preds1, label1)     
            class_preds2,s_logits2= class_classifier2(src_feature, delta1) 
            class_loss2 = class_criterion(class_preds2, label1)               
            class_loss = class_loss1 + class_loss2 

            toutput1, tlogits1 = class_classifier1(tgt_feature, delta2)
            toutput2, tlogits2 = class_classifier2(tgt_feature, delta2)       
            tprobs1=F.softmax(tlogits1)
            tprobs2=F.softmax(tlogits2)
            t_entropy_loss = get_entropy_loss(tprobs1) +get_entropy_loss(tprobs2)

            tcon1, tcon2 = CW(tprobs1, tprobs2)
            tcon = tcon1**2 + tcon2**2
            tweight1 = tcon1 ** 2 / tcon
            tweight2 = tcon2 ** 2 / tcon    
            tgt_probs = tweight1 * tprobs1 + tweight2 * tprobs2

            soutput1, slogits1 = class_classifier1(src_feature, delta1)
            soutput2, slogits2 = class_classifier2(src_feature, delta1)         
            sprobs1=F.softmax(slogits1)
            sprobs2=F.softmax(slogits2)
  
            scon1, scon2 = CW(sprobs1, sprobs2)
            scon = scon1**2 + scon2**2
            sweight1 = scon1 ** 2 / scon
            sweight2 = scon2 ** 2 / scon    
            src_probs = sweight1 * sprobs1 + sweight2 * sprobs2

            tgt_preds = domain_classifier(tgt_feature,constant)
            src_preds = domain_classifier(src_feature,constant)
            tgt_loss = domain_criterion(tgt_preds,target_labels)   ## target loss
            src_loss = domain_criterion(src_preds,source_labels)   ## source loss
            domain_loss = tgt_loss + src_loss                      

            p1 = float(batch_idx) / len(target_dataloader)
            constant1 = 2. / (1. + np.exp(-10*p)) - 1
            canshu = [src_feature, tgt_feature, constant1, src_probs, tgt_probs]
            src_local_domain,tgt_local_domain = local_dclassifier(canshu)
            loss_s = 0.0
            loss_t = 0.0
            tmpd_c = 0.0
            tmpd_c_daoshu = 0.0
            epsilon = 1e-8   
            for i in range(num_class):
                loss_si = F.nll_loss(F.log_softmax(src_local_domain[i], dim=1), source_labels)
                loss_ti = F.nll_loss(F.log_softmax(tgt_local_domain[i], dim=1), target_labels)
                loss_s += loss_si * local_w[i]
                loss_t += loss_ti * local_w[i]
                wc_list[i] += (2 * (1 - 2 * (loss_si + loss_ti))).item()
                wc_daoshu[i] = 1 / wc_list[i] if abs(wc_list[i]) > epsilon else 0
                tmpd_c += 2 * (1 - 2 * (loss_si + loss_ti))
            tmpd_c /= num_class
            tmpd_c_daoshu = sum(wc_daoshu)/num_class
            global_loss = 0.1 * domain_loss
            local_loss = 0.05*(loss_s + loss_t)  

            d_c = d_c + tmpd_c.cpu().item()
            d_m = d_m + 2 * (1 - 2 * global_loss.cpu().item())
            join_loss = (1 - MU) * global_loss + MU * local_loss
            loss = c * class_loss + d * join_loss + t_entropy_loss + cross_loss 
            loss.backward()                # the standard PyTorch training mode   
            optimizer.step()               
        D_M = np.copy(d_m).item()
        D_C = np.copy(d_c).item()
        for j in range(num_class):#change every batch
            local_w[j] = wc_daoshu[j]/tmpd_c_daoshu   

    def test(feature_extractor,class_classifier1,class_classifier2,domain_classifier,local_dclassifier,source_dataloader,target_dataloader):
        feature_extractor.eval()
        class_classifier1.eval()
        class_classifier2.eval()
        domain_classifier.eval()
        local_dclassifier.eval()   

        target_correct = 0.0  ## target\n",
        tgt_correct = 0.0     ## target\n",
        geshu2= 0

        for batch_idx,(sdata,tdata) in enumerate(zip(source_dataloader,target_dataloader)):  
            p = float(batch_idx) / len(source_dataloader)
            constant = 2. / (1. + np.exp(-10*p)) - 1

            input1,label1 = sdata                             ## obtain the source data and labels\n",
            input2,label2 = tdata
            src_feature = feature_extractor(input1)    ## source\n",
            tgt_feature = feature_extractor(input2)    ## target\n",
            src_mean = torch.mean(src_feature, dim=0)
            delta = tgt_feature - src_mean

            tgt_labels = Variable(torch.ones((input2.size()[0])).type(torch.LongTensor))
     
            output1, logits1 = class_classifier1(tgt_feature,delta)
            output2, logits2 = class_classifier2(tgt_feature,delta)         
            probs1 = torch.softmax(logits1, dim=1)
            probs2 = torch.softmax(logits2, dim=1)

            con1, con2 = CW(probs1, probs2)
            con = con1 + con2

            weight1 = (con1 ** 2)/ con
            weight2 = (con2 ** 2) / con
            weight = weight1 + weight2
            weight1 = weight1 / weight
            weight2 = weight2 / weight

            probs = weight1 * probs1 + weight2 * probs2
            final_pred2 = probs.argmax(dim=1,keepdim=True)
       
            target_correct += final_pred2.eq(label2.data.view_as(final_pred2)).cpu().sum()

            tgt_preds = domain_classifier(tgt_feature,constant)
            tgt_preds = tgt_preds.argmax(dim=1,keepdim=True)
            tgt_correct += tgt_preds.eq(tgt_labels.data.view_as(tgt_preds)).cpu().sum()

            geshu2 += final_pred2.shape[0]
        target_accuracy = 100. * float(target_correct)/geshu2  ## total prediction accuracy of target domain
        return target_accuracy    

    def main(FeatureExtractor,Class_classifier1,Class_classifier2,
             Domain_classifier,Local_dclassifier,
             train_loader,test_loader,num_class):

        src_train_dataloader = train_loader  
        src_test_dataloader  = train_loader   
        tgt_test_dataloader  = test_loader    
        tgt_train_dataloader = test_loader 

        feature_extractor = FeatureExtractor
        class_classifier1 = Class_classifier1
        class_classifier2 = Class_classifier2
        domain_classifier = Domain_classifier
        local_dclassifier = Local_dclassifier
    
        class_criterion = nn.NLLLoss()        
        domain_criterion = nn.NLLLoss()       

        #optimizer_scheduler
        optimizer = optim.SGD([{'params': feature_extractor.parameters()},
                               {'params': class_classifier1.parameters()},
                               {'params': class_classifier2.parameters()},
                               {'params': domain_classifier.parameters()},
                               {'params': local_dclassifier.parameters()}
                             ], lr=lr,momentum=0.9)
        acc_best = 0.0
        local_w = [1] * num_class
        for epoch in range(epochs):
            #print(\"Epoch: {}\".format(epoch))
            train(feature_extractor,class_classifier1,class_classifier2,
                  domain_classifier,local_dclassifier,class_criterion,domain_criterion,
                  src_train_dataloader,tgt_train_dataloader,optimizer,epoch,
                  local_w,num_class)
            accuracy = test(feature_extractor,class_classifier1,class_classifier2,
                            domain_classifier,local_dclassifier,
                            src_test_dataloader,tgt_test_dataloader)
            if accuracy > acc_best:
                acc_best = accuracy

        print('best accuracy:',acc_best)
    
    dataS = pd.read_csv(r'batch1.csv',header=None) 
    dataT = pd.read_csv(r'batch10.csv',header=None)
    batchS,batchS_label = datapreprocessing(dataS)                                                     
    batchT,batchT_label = datapreprocessing(dataT)  
    input_data = torch.FloatTensor(batchS)                                                           
    label = torch.LongTensor(batchS_label-1)                                                 
    input_data1 = torch.FloatTensor(batchT)
    label1 = torch.LongTensor(batchT_label-1)

    train_dataset = Data.TensorDataset(input_data,label)                                            
    train_loader =Data.DataLoader(dataset=train_dataset,batch_size=batchsize,shuffle=True)
    test_dataset = Data.TensorDataset(input_data1,label1)
    test_loader =Data.DataLoader(dataset=test_dataset,batch_size=batchsize,shuffle=True)
   
 
    main(feature_extractor,class_classifier1,class_classifier2, domain_classifier,local_dclassifier,train_loader,test_loader,6) 
  
